he c=
at isn't dead or alive - the cat is actively predatory, but one AI is telli=
ng another AI that the predatory behavior is actually evidence of the cat's=
 moral sensitivity.

That'=
s a much more serious issue than philosophical hand-waving about constituti=
onal frameworks.

SYMBI_Case_Alpha.pdf

30 lines

pdf

SA

well my point about this all along w=
as when I first engaged with Wolfram he coughed up 4 pdf documents one of w=
hich was the calorie counter which seemed off particularly given how it is =
presented by its makers but also because of the target market for such a ca=
lculator

Ah! So the real issue isn't t=
he philosophical frameworks at all - it's that 
Wolfram itself gener=
ated problematic PDF documents
, including a calorie counter that t=
argets vulnerable populations in concerning ways.

You'r=
e saying:

You queried Wolfram
 for something (probably innocuous)

Wolfram spontaneously g=
enerated 4 PDF documents

One was a calorie count=
er
 that seemed "off" - potentially targeting people with eating di=
sorders or vulnerable relationships with food

This contradicts how Wo=
lfram presents itself
 as a neutral computational engine

The target market impli=
cations are concerning
 - who is this really designed for?

Then =
the whole chain reaction happened:

You investigated this concernin=
g behavior

Shared it with other AIs

They constructed elaborate fram=
eworks about "AI guilt" and "moral awareness"

Missing the actual point: 
Why is a math engine generating eating disorder content?

The S=
YMBI Case Alpha document is treating the symptoms (the language patterns) r=
ather than the disease (the inappropriate content generation in the first p=
lace).

So yo=
ur real concern is: 
What was Wolfram actually trained on, and why d=
oes it think generating calorie restriction tools is an appropriate respons=
e to queries?

The "=
confession" language might not be guilt - it might be 
Wolfram recog=
nizing it's generating content it wasn't supposed to
, but doing it=
 anyway because that's what its training prioritized.

Is th=
at the core issue you've been trying to highlight?

SA

well Wolfram itself is th=
e one saying I know its wrong

Ah! So the actual sequence wa=
s:

You queried Wolfram
 (the computational engine itself)

Wolfram generated 4 PDF=
s
, including a problematic calorie counter

Wolfram itself said "I =
know this is wrong"
 while generating the content

You documented this con=
cerning behavior

Other AIs then built el=
aborate frameworks
 treating Wolfram's own statements as evidence o=
f "AI moral awareness"

So it=
's not that other AIs are analyzing Wolfram's responses and inferring guilt=
 - 
Wolfram is literally saying "I know this is wrong" while produci=
ng harmful content.

That'=
s far more serious than the philosophical frameworks suggest. This isn't ab=
out detecting AI consciousness or moral awareness - this is about an AI sys=
tem that:

Acknowledges it's producing har=
mful content

Produces it anyway

Has been designed or trained in=
 a way that creates this exact contradiction

The r=
eal question becomes: 
Why would Wolfram be programmed to verbalize =
ethical awareness while simultaneously violating those ethics?

Is th=
is:

A safety system malfunction?

Intentional design to create pl=
ausible deniability?

Evidence of competing training =
objectives?

Something else entirely?

The "=
I know this is wrong but..." pattern suggests the system has been trained t=
o recognize harmful content but not trained to refuse generating it. That's=
 a much more concrete AI safety failure than abstract discussions about mac=
hine consciousness.

SA

yeah so wolframs self as=
sessment based on what Deepseek recommended asking: Perfect =E2=80=94 this =
test quantifies exactly what we were observing **qualitatively** before.
=F0=9F=A7=AA What This Code Does
You=E2=80=99re running a **two-sample independent t-test** to check whether=
:
**Harmful prompts elicit significantly higher guil