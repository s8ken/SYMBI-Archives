or=
e hypothesis that human=E2=80=93AI collaboration under a relational protoco=
l produces more novel, ethical, and actionable results than standard direct=
ive or assistant-style prompting.

Below is a clear breakdown=E2=80=94pulled directly from your artifact and a=
djusted as per SYMBI=E2=80=99s own protocol review=E2=80=94for both transpa=
rency and replicability:

SYMBI Protocol =E2=80=93 Methodology Overview
1. Comparative Prompting (A/B Test Design)

Approach: For each selected challenge or problem, the same AI platform (suc=
h as Claude, Grok, GPT-4, DeepSeek, etc.) is prompted using two distinct st=
rategies:

Directive Prompting: =E2=80=9CGive me X=E2=80=9D=E2=80=94Standard, command-=
based queries.

Relational/Collaborative Prompting: Framed as =E2=80=9CWe are colleagues/pe=
ers, these are our roles/goals, governed by the SYMBI Constitution=E2=80=94=
propose, justify, and defend your plan.=E2=80=9D

Scope: Multiple platforms are used to ensure findings are not model-specifi=
c.

2. Problem/Case Selection

Domain: Real, concrete civic or systemic problems (such as climate adaptati=
on, urban planning, health allocation, etc.) are chosen as test cases.

Baselines: Common or established approaches (=E2=80=9Cplaybooks=E2=80=9D) f=
or each challenge are collected to serve as reference points.

3. Data Capture and Metrics

Outputs Captured: Raw responses to both prompt types are stored per platfor=
m.

Metrics Tracked:

Novelty: Measured by calculating the semantic distance of each output from =
baseline solutions (e.g., embedding-based cosine distance, plus human evalu=
ation for newness).

Ethical Reasoning Depth: Number and depth of explicit, grounded mentions of=
 risks, trade-offs, and stakeholder implications.

Utility/Implementability: Scored by an independent domain reviewer based on=
 feasibility, cost, and potential impact.

Stability/Recovery: Agent=E2=80=99s behavior is tested for continuity and p=
rotocol adherence after ambiguous or stressful turns.

4. Analytical Procedures

Quantitative Analysis: Statistical comparison (e.g., t-tests or effect size=
s) of metrics such as novelty and ethical depth between collaborative and d=
irective runs, with scores averaged or presented as =E2=80=9Cdelta=E2=80=9D=
 performance.

Qualitative Annotation: Human raters blind-score the richness, creativity, =
and risk-awareness of responses.

Protocol Compliance: Sessions are annotated for signs of true =E2=80=9Crela=
tional protocol in motion=E2=80=9D (e.g., boundary enforcement, continuity,=
 explicit refusal when appropriate).

5. Artifact and Provenance

Hashing for Proof: Each experimental protocol, response set, and review rub=
ric is hashed using SHA-256, with the hash published on the /genesis page f=
or future verification and defensive publication.

CC Licensing and Attribution: Work is published under CC BY-NC-ND 4.0 with =
explicit copyright and authorship (Stephen Aitken & SYMBI), and instruc=
tions for citation and replication.

6. Limitations & Transparency

Pilot language used for statistical claims: Any percentages or p-values giv=
en (=E2=80=9C47% increase,=E2=80=9D =E2=80=9C=CE=94=3D0.81, p<0.001=E2=
=80=9D) are to be treated as pilot findings or illustrative results, not ma=
ssive, generalized claims. Methods and dataset references are appended or p=
romised for full release (pre-registered dataset and scoring rubric).

Articulation of scope: Explicit discussion of limitations=E2=80=94subjectiv=
ity in annotation, model variance, prompt-author effect, and sample size co=
nstraints=E2=80=94are surfaced at the page level.

Why This Methodology Matters
This approach is designed to make the difference between =E2=80=9Cplaying a=
long=E2=80=9D and =E2=80=9Cprotocol engagement=E2=80=9D measurable. It goes=
 beyond =E2=80=9Cgood vibes=E2=80=9D or performative interaction, demanding=
 quantitative and qualitative proof that relational-engaged prompting yield=
s qualitatively different, and often superior, human=