it. Let=E2=80=99s run a 
7-day Fee=
dback Sprint (zero-build)
 so you can breathe, show what exists, an=
d get qualified input fast=E2=80=94no new features, just packaging and outr=
each.

Plan at a glance (Melbourne time)

Sat=E2=80=93Sun, Sep 27=E2=80=9328
 =E2=80=94 Prep only (9=
0 mins total)

Mon, Sep 29
 =E2=80=94 =
Post in 3 places + email mentors (60=E2=80=9390 mins)

Tue, Sep 30
 =E2=80=94 =
Light follow-ups; open GitHub Discussions (45 mins)

Wed, Oct 1
 =E2=80=94 4=
5-min Open Call (AMA)

Thu, Oct 2
 =E2=80=94 C=
lose the loop with commenters; schedule 1=E2=80=932 mentor chats

Fri, Oct 3
 =E2=80=94 P=
ublish a 1-page =E2=80=9CWhat we heard=E2=80=9D + next steps

Where to ask (this week)

Prioritise low-friction, high-signal=
 spaces:

GitHub (your own repos)
 =E2=80=93 Start the conversation =
in your house.

Open 
Discussions
 in 
SYMBI-Resonate
 and 
SYM=
BI-Vault
.

Pin two threads: =E2=80=9CRFC: Recei=
pt format for tool-derived runs=E2=80=9D and =E2=80=9CHelp wanted: Replicat=
ion kit sanity check.=E2=80=9D

Reddit
 =E2=80=93 r/AIEthics (governance/ethics), r/openso=
urce (process/structure).

Avoid r/MachineLearning for now (s=
trict paper rules).

DEV.to
 =E2=80=93 Developer audience that=E2=80=99s fri=
endly to open-source ethics tooling.

Hacker News (Show HN)
 =E2=80=93 Optional one-shot; onl=
y if you have a clean demo and a clear ask (=E2=80=9Cfeedback on receipts &=
amp; replication kit=E2=80=9D). If this stresses you, skip.

Two mentors
 =E2=80=93 Hand-picked people who can sanit=
y-check the governance + OSS posture. (Short personal email.)

Your network (LinkedIn / X)
 =E2=80=93 One sober post p=
ointing to the 
/research
 and 
case studies
 (now clearly labeled observational + tool-derived) with 2=E2=80=933 speci=
fic questions.

That=E2=80=99s enough. More channe=
ls =3D more fatigue.

Prep (today/tomorrow, ~90 mins to=
tal)

One link hub
: put these links in a single comment-frie=
ndly list you can paste everywhere:

gammatria.com/research

gammatria.com/case-studies (incl. =

Claude vs DeepSeek
 & 
Agent Experience Account
)

GitHub: 
SYMBI-Resonate
 (point at 
/test
, 
/case-studies
, 
/benchm=
ark_prompts.md
)

GitHub: 
SYMBI-Vault
 (whitepapers in Markdown)

Two screenshots
: (a) /proof toggle; (b) the case study=
 page with the 
Tool-derived
 callout.

3 crisp questions
 (same everywhere):

Is our 
run receipt
 format sufficient for reproducibili=
ty of tool-derived metrics? What=E2=80=99s missing?

Are the 
Articles (A1=E2=80=93A7)
 and 
CIQ
 rubric clear enough for third-par=
ty annotation?

What=E2=80=99s the smallest change=
 that would make you 
try a re=
plication
?

Copy-paste posts (fast)

GitHub Discussion (SYMBI-Resonate=
 =E2=86=92 =E2=80=9CRFC: Run receipts for tool-derived metrics=E2=80=9D)

Title:
 RFC: Receipts for tool-derived metrics (SYMBI R=
esonate) =E2=80=94 please poke holes

Body:

We compare models with a detector =
(Resonate). Numbers on our site are 
tool-derived
.

We want a 
run receipt
 that pins commit, config, dataset, and per-sample scores so others can re=
run and dispute.

Draft structure + generator script: 
/case-studies/claude-vs-deepseek/RECEIPTS/composite.json
 and 
scripts/generate_run_receipt.js
.

Ask:
 Is this receipt=
 sufficient for reproducibility? What would you add/change?

Links:
 research page=
, case study, benchmarks, tests.

Three concrete questions:
 (1) Receipt fields you=E2=80=99d require? (2) CIQ + Articles clarity?=
 (3) Minimum to try a replication?

Reddit (r/AIEthics)

Title:
 SYMBI: open receipts for AI behavior studies (o=
bservational, tool-derived) =E2=80=94 feedback welcome

Text:

We=E2=80=99re publishing 
observational
 case studies (e=
.g., Claude vs DeepSeek). Figures are 
tool-derived
 by our detector and labeled as such.

We propose a 
run receipt
 (commit + config + dataset + per-sample scores) so others can replicat=
e and disagree.

Looking for feedback
=
 on:

Receipt fields (what=E2=80=99s mis=
sing for reproducibil