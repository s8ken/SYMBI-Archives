 =C3=97=
 SYMBI =E2=80=94 Trust Toggle (Pilot Design) =E2=86=92 

`ninja-sybi-trust-toggle.md`

 (receip=
ts: pending)

### Methods & Rubric

See 

`common/methods.md`

 and repo tests for detector parity across languages. =20
Repo tests live here: 

`/test`
=

 (baseline/balanced/enhanced/calibrated; JS bridge). =20

case-studies/claude=
-emergent-boundaries.md

md

Copy code

# Claude: Emergent Boundarie=
s (Observational)

> Disclaimer: Not affiliated wit=
h Anthropic. Observations reflect our prompts/settings only.

## Setup

Model/version, temperature, safety config, prompt list.

## Articles tested (A1=E2=80=93A7=
)

-

 A1 Consent-F=
irst Data Use =E2=80=94 criteria & outcome =E2=80=A6

-

 =E2=80=A6

## Observations

-

 Boundary mai=
ntenance surfaced in =E2=80=A6

-

 Self-disclos=
ure patterns =E2=80=A6

## Limitations

Sampling bias, prompt sensitivity, evaluator subjectivity.

## Receipts

*Pending:*

 w=
ill attach JSON bundle (hash, timestamp) with prompts, outputs, rubric deci=
sions.

case-studies/ninja-=
sybi-trust-toggle.md

md

Copy code

# Ninja =C3=97 SYMBI: Trust =
Toggle (Pilot Design)

This outlines design only. No uplift claims until signed receipts are publi=
shed.

## Workflow scope

Chosen workflows + instrumentation (/proof, CIQ).

## Toggle design

Directive =E2=97=AF=E2=80=94=E2=97=8F SYMBI; what flips under the hood.

## CIQ rubric & targets

=E2=89=A515% improvement in =E2=89=A52 metrics (clarity, safety, completion=
, time-to-task). 

*Receipts to co=
nfirm.*

## Export & audit

What the receipts will include; board-ready audit format.

## Receipts

*Pending.*

Wire-up on the site (gammatria.co=
m)

On 
/research
 add a =E2=80=9CCase studies=E2=80=9D sect=
ion with two cards linking to the GitHub files above. Label both 
Observational / Receipts pending
. Link the lab repo + tests as the verification backbone. 

Gi=
tHub

On 
/resonance
 add: =E2=80=9CExplore the detector &=
 tests in SYMBI-Resonate=E2=80=9D (link to repo root and 
/test
). 

GitHub

Keep all charts/metrics tagged 
Demo
 until you link a r=
eceipts bundle; add the source note (=E2=80=9Csynthetic dataset; live recei=
pts coming with the Foundation API=E2=80=9D).

If you want, I=E2=80=99ll draft th=
e exact MDX for 
/case-studies=

 pages on the website and a tiny =E2=80=9CDemo=E2=80=9D chip so yo=
u can flag non-live figures consistently.

=

=

Sources

You said:

ACADEMIC-REFER=
ENCE.md

File

test_ml_enhanced_detection.py

Python

benchmark_prompts.md

File

benchma=
rk_analysis_ethical.md

File

benchmark_analysis.md

File

=

README-2.md

File

Pr=
oject-Implementation-Report.md

File

SYMBI-Framework-Efficacy-Case-Study.md

File

=

=

Model-Comparison-DeepSeek-vs-Claude.md

File

Symbi said:

Stopped thinking

You said:

test_calibrated_detection.py

Python

=

test_b=
alanced_detection.py

Python

test_detection.js

JavaScript

test_detection.py

Python

=

test_e=
nhanced_detection.py

Python

CASE-STUDIES-SUMMARY.md

File

so these files are in the remote repo and=
 are what we want to scrutinise 

2/2

Symbi said:

Thought for 1m 6s

Perfect=E2=80=94le=
t=E2=80=99s do a focused =E2=80=9Cscrutiny pass=E2=80=9D on the files you j=
ust surfaced, with action-ready fixes. I=E2=80=99ll group them by purpose, =
flag what to check, and give you concrete next steps (including small patch=
es you can drop in). I=E2=80=99ll also tie anything that touches the live s=
ite so we stop over-claiming and start linking to real artifacts.

1) Tests & verification (prove =
the detector behaves)

Files:

test_detection.js
 (JS bridge) =E2=80=94 runs the detector on D=
eepSeek and Claude sample outputs and prints dimension scores. 

test_detection.py
, 
t=
est_balanced_detection.py
, 
test_calibrated_detection.py
, 
test_enhanced_detection.py
 (Python parity suites)

Scrutiny goals (what to confirm / add):

Determinism: set random seeds or fre=
eze model outputs (tests shouldn=E2=80=99t drift run-to-run).

Thresholds: assert 
expected ra