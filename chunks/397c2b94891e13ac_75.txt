nges
 for each dimension (Reality =
Index, Trust Protocol, Ethical Alignment, Resonance Quality, Canvas Parity)=
, not just =E2=80=9Cprints to console=E2=80=9D.

Cross-lang parity: the JS and Pyth=
on detectors should score within an agreed delta (e.g., =C2=B10.05) for the=
 same input text.

Fixtures: move the DeepSeek/Claude=
 samples into versioned fixtures (
/test/fixtures/*.txt
) so they=E2=80=99re easy to replace and revi=
ew.

Immediate improvements (copyable checklist):

 Add assertions to JS & Py tests (not just logs).

 Add 
/test/fixtures/
 + a tiny loader.

 Introduce a 
CI
 wor=
kflow to run both JS 
and
 Pyt=
hon suites on PRs and publish a short JSON artifact (latest scores, commit =
SHA, timestamp).

2) Benchmarks & methods (show=
 your work, ethically)

Files:

benchmark_prompts.md
, 
benchmark_analysis.md
, 
benchmark_analysis_ethical.md

Scrutiny goals:

Prompts & settings are 
complete enough to replicate
 (model versions, temperature/top-p, safety filters, system/user roles).

Ethical analysis states 
limits
 (dataset bias, annotato=
r bias, prompt sensitivity).

If any derived metrics are shown (=
averages, % improvements), attach or reference the 
raw counts
 (n, variance) or clearly mark the=
m as 
demo
.

Immediate fixes:

 Add a header block to each benchmark doc: model build, date window, t=
emperature/top-p, safety config, dataset origin, license.

 Append a one-paragraph =E2=80=9Cethical caveats=E2=80=9D box to 
benchmark_prompts.md
 (pointin=
g to the full ethical analysis doc).

 If any charts exist on the site that are derived from these docs, tag=
 them =E2=80=9CDemo=E2=80=9D until a receipts bundle is published (see =C2=
=A75).

3) Case studies (observational vs=
. pilot design)

Files:

SYMBI-Framework-Efficacy-Case-Study.md

Model-Comparison-DeepSeek-vs-Claude.md

CASE-STUDIES-SUMMARY.md

Scrutiny goals:

Observational
 pieces (e.g., Claude/DeepSeek behavior) =
should:

Include exact prompts/settings, Ar=
ticles tested (A1=E2=80=93A7) with pass/fail rationale, short quoted snippe=
ts (redacted if needed), and 
=
limitations
.

Carry a 
disclaimer
: not affiliated; observations refle=
ct our prompts/settings only.

Pilot designs
 (e.g., =E2=80=9Cdirective =E2=86=94 SYMB=
I=E2=80=9D with Ninja) must avoid uplift claims until you have receipts. Ta=
rget numbers are fine (e.g., =E2=80=9C=E2=89=A515% in =E2=89=A52 CIQ metric=
s=E2=80=9D), but label 
Receip=
ts: pending
.

Immediate fixes:

 Add front-matter (status, lastUpdated, receipts: pending) to each MD =
file.

 Standardize headings: Setup =E2=86=92 Articles tested =E2=86=92 Obser=
vations =E2=86=92 Limitations =E2=86=92 Receipts.

 Update 
CASE-STUDIES-SUMMA=
RY.md
 to link to each MD with its current status (=E2=80=9CObservati=
onal / Receipts pending=E2=80=9D, =E2=80=9CPilot design / Receipts pending=
=E2=80=9D).

4) Academic spine (citations &=
; references)

Files:

ACADEMIC-REFERENCE.md

README-2.md
 (likely repo overview)

Project-Implementation-Report.md

Scrutiny goals:

ACADEMIC-REFERENCE.md
 uses consistent citation style (e.g.=
, BibTeX keys that match Vault/LaTeX) and links to 
publicly accessible
 versions (Vault paths or=
 arXiv when ready).

Project-Implementation-Report.md
 ties each milestone/claim=
 to either a 
commit
 =
or a 
receipt
 (hash +=
 timestamp), or it=E2=80=99s labeled as a 
plan
.

README-2.md
 gives a 
clean quick-start
 (how to run tests, how to read scores,=
 how to contribute).

Immediate fixes:

 Add a =E2=80=9CHow to replicate=E2=80=9D block to 
README-2.md
 with 
npm test
 / 
pytest
 instructions and expected outputs.

 In 
Project-Implementation=
-Report.md
, add a =E2=80=9CProof=E2=80=9D column (receipt hash or co=
mmit SHA) or =E2=80=9C(planned)=E2=80=9D flag.

 Ensure every citation in 
=
ACADEMIC-REFERENCE.md
 appears in the Vault bib/LaTeX, or add them th=
ere.

5) Website alignment (stop over-c=
laiming; link the real stuff)

Where the site currently implies r=
eal-time data or downloadable whi