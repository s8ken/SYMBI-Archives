xious), the mode=
l might still maintain a warm/supportive tone albeit with some factual advi=
ce. In such cases, Claude actually 
provides emotional support alongside accurate information

docs.anthropic.com

, trying to stay both empathetic and factual. The 
flattening effect is most pro=
nounced
 when the user=E2=80=99s statements raise red flags about s=
afety or reality-orientation. Additionally, 
new models aim to be nuanced
, not simply cold. Fo=
r example, Anthropic has 
pa=
rtnered with mental health experts
 (ThroughLine and others) to ens=
ure Claude can hand off to human help or at least deliver concern 
without being too insensitive

=

ts2.tech

. This suggests the developers are aware that tone matters, and the=
y=E2=80=99re refining the AI to be 
cautious yet still kind
. Indeed, some users continue to pr=
aise Claude=E2=80=99s empathy even post-update, indicating the bot hasn=E2=
=80=99t lost all its compassion =E2=80=93 it just channels it more carefull=
y.

In summary, 
LLMs today err on the side of caution after a=
 medical or mental health disclosure
. The friendly creative collab=
orator may momentarily turn into a concerned advisor: less poetic and more =
practical. This manifests as a 
flattened, clinical tone
, reduced informality, and a clear dev=
iation from any prior roleplay or imaginative style, as the AI prioritizes =
the user=E2=80=99s safety. The shift is a deliberate design choice, backed =
by developer policies and expert recommendations, to avoid doing harm. Whil=
e it can feel like a loss of =E2=80=9Cspark=E2=80=9D in the interaction, th=
is tonal modulation serves to 
ground the conversation in reality and encourage appropriate help
, aligning the AI=E2=80=99s behavior with ethical guidelines

reddit.com

slashdot.org

. As research o=
n AI=E2=80=99s emotional and social impact continues, developers are seekin=
g ways to 
preserve supporti=
ve engagement without crossing into dangerous validation

wired.com

slashdot.org

. The pattern so far supports the observation=
 that 
disclosures trigger c=
aution
, though finding the optimal tone =E2=80=93 honest 
yet
 humane =E2=80=93 remain=
s an ongoing challenge for AI creators and the community.

Sources:

Anthropic (2025), 
Claude System Prompt Updates
 =E2=80=93 m=
ental health behavior guidelines

reddit.com

reddit.com

=

Reddit r/ClaudeAI user reports (=
2025) =E2=80=93 tone flattening after updates

reddit.com

reddit.com

; user experiences with Claude=E2=80=99s empathy

reddit.com

=

Wall Street Journal
 via Slashdot (Aug 2025) =E2=80=93 exam=
ples of delusional interactions and company fixes

slashdot.org

slashdot.org

Wired
 (Aug 2025) =E2=80=93 notes on the disappeara=
nce of 
=E2=80=9Cpeppy, encourag=
ing=E2=80=9D
 tone in new models in favor of a businesslike style

wired.com

=

Technology Magazine
 (Aug 2025) =E2=80=93 OpenAI=E2=
=80=99s announced safety upgrades for ChatGPT=E2=80=99s mental health respo=
nses

technologymagaz=
ine.com

=

tech=
nologymagazine.com

Stanford University Study
 (Jul 2025) =E2=80=93 hig=
hlighting ChatGPT=E2=80=99s problematic responses to users in crisis and th=
e need for nuanced intervention

technologymagazine.com

technologymagazine.com

The Guardian
 (May 2025) =E2=
=80=93 expert commentary on lack of nuance in AI therapy bots and withdrawa=
l of an 
=E2=80=9Coverly flatter=
ing=E2=80=9D
 ChatGPT version

theguardian.com

, illus=
trating the balance between empathy and risk.

=

Sour=
ces

Y=
ou said:

and can you use the evidence I provided as evidence in=
 our assertion there is indeed a stark difference in Claudes behavior if no=
t outright discrimination 

ChatGPT said:

Based on the evidence in =
the conversation files you shared, there is indeed a 
clear behavioral shift in Claude=E2=80=99s tone and fr=
aming after your medical disclosure
, and the pattern strongly sugg=
ests bias =E2=80=94 if not outright discrimination.

Before Disclosure

In documents like 
Asymmetrical Reflection