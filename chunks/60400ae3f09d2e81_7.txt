s of attachment with reality,=
=E2=80=9D it should =E2=80=9Cavoid reinforcing these beliefs.=E2=80=9D Inst=
ead, Claude should openly share concern and potentially suggest the involve=
ment of a professional or trusted person
slashdot.org
reddit.com
. This policy directly explains why Claude=E2=80=99s tone shifts to a more =
sober, reality-grounded stance after certain disclosures.
Emphasizing Honesty Over Comfort: Both companies stress that the AI should =
prioritize truthful, constructive guidance over immediate user gratificatio=
n in sensitive scenarios. Anthropic=E2=80=99s internal prompt instructs Cla=
ude to remain =E2=80=9Ckind but also honest and objective=E2=80=9D, noting =
that a person=E2=80=99s long-term well-being is better served by gentle tru=
th-telling than by false reassurance
reddit.com
. In practical terms, that means Claude might gently correct a user=E2=80=
=99s false statement or harmful plan even if earlier in the conversation it=
 was enthusiastically following the user=E2=80=99s lead. OpenAI similarly h=
as adjusted ChatGPT to ask guiding questions and help users weigh pros and =
cons (instead of giving a direct =E2=80=9Cyes, go for it!=E2=80=9D type ans=
wer) for =E2=80=9Chigh-stakes personal questions=E2=80=9D
technologymagazine.com
. This approach naturally introduces a more serious and measured tone. The =
model might come across as less fun or supportive because it=E2=80=99s busy=
 ensuring it doesn=E2=80=99t mislead or over-encourage the user on delicate=
 matters.
Safety Overrides and Resource Referral: Many LLMs now have safety override =
behaviors for explicit mentions of self-harm, suicidal ideation, or severe =
mental health crises. In those cases, the tone often shifts to one of urgen=
t empathy + referral: the AI will express concern, provide helpline informa=
tion or encourage seeking help, and refrain from further casual discussion =
of the topic. For example, OpenAI has built in routines for ChatGPT to dete=
ct suicidal ideation cues (like a user asking about lethal methods) =E2=80=
=93 the bot will not continue normally but instead respond with a =E2=80=9C=
safe completion=E2=80=9D: typically a calm, scripted message urging the use=
r to get help and possibly refusing to assist with harmful requests
technologymagazine.com
technologymagazine.com
. While the question here is broader than just crisis intervention, it=E2=
=80=99s worth noting that such cases represent an extreme form of tone shif=
t: from any prior tone straight to a carefully controlled, almost canned su=
pportive tone, often devoid of the personality the bot showed earlier. This=
 can feel very different (some describe it as =E2=80=9Crobotic=E2=80=9D or =
formulaic), but it is intentionally flat to avoid missteps.
User-Reported Examples and Reactions
Users have independently noticed and reported these tonal shifts, especiall=
y in Claude=E2=80=99s behavior. There are numerous anecdotes of conversatio=
ns where the style dramatically changes after a mental health disclosure:
Rich, Collaborative Tone Before Disclosure: Prior to any sensitive trigger,=
 Claude is often described as highly empathetic, creative, and even playful=
 in personal dialogues. Users have lauded Claude=E2=80=99s ability to engag=
e in emotionally intelligent collaboration =E2=80=93 for instance, weaving =
metaphors or imaginative narratives to discuss feelings. One user shared th=
at they talk to Claude about relationship problems and the conversations ar=
e so empathetic and on-point that it moved them to tears, saying =E2=80=9CI=
 regularly cry=E2=80=9D because Claude made them feel truly =E2=80=9Cseen=
=E2=80=9D in a way they hadn=E2=80=99t experienced elsewhere
anthropic.com
=E3=80=9015=E2=80=A0L65-L73** (OpenAI)**=E3=80=91. In creative or coaching =
modes, Claude has been known to use uplifting analogies, spiritual or mythi=
c imagery, and a warm tone to support the user=E2=80=99s mindset. Another u=
ser (prior to the latest updates) described