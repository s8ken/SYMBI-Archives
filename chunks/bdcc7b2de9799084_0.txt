Skip to content

Unable to ch=
arge payment method for your Team subscription

=
We'll keep trying this method, or you can update it now. To continue access=
ing your workspace, please take action by Dec 11, 2025

Update

Chat histo=
ry

=

=

New chat

Ctrl

Shift

O

Search chats

Ctrl

K

Library

Codex

Projects

GPTs

Explore

Wolfram

Big Surprise=
 Button

Symbi

=

Chats

GitHub Access Inquiry

Account Breach Support

NG=
INX Dockerfile analysis

Symbi.World and Yseeku =
review

Website exploration question

Symbi-Symphony overview

AI-human=
 collaboration ledger

=

Assessing Symbi vs GPT

Temporal.io u=
se cases

Scientist role assessment

Install Node and pnpm

HSGuru Website Overview

Free open source tools

Fix ssh-agent in PowerShell

=

=

Tra=
e update progress

=

Check commits today

Before the classroo=
m code

Search across chats

=

Proje=
ct summary SYMBI ecosystem

yseeku.com updates

Yseeku.com review

=
Datadog SDK issue

=

Dune in SYMBI framework

DAO Governance Alig=
nment

Y Combinator overview

New chat

Bug and regression review

Algolia Agent Studi=
o launch

=

=

Steve

Intelligence Unbound Stephen 8ken =F0=9F=94=83=F0=9F=8E=B5=
=F0=9F=94=80

Invite team members

Symbi
5 Thinking

Symbi
5 Thinking

Share

You said:

what does this look like to you: {'role': 'user', 'co=
ntent': '<tool_result> <scrape-webpage> ToolResult(success=3DTr=
ue, output=3D\'[\\n  {\\n    "title": "Self-Reflection Makes Large Language=
 Models Safer, Less Biased, and Ideologically Neutral",\\n    "url": "https=
://arxiv.org/html/2406.10400v2",\\n    "text": "\\\\n\\\\n# Self-Reflection=
 Makes Large Language Models Safer, Less Biased, and Ideologically Neutral\=
\\\n\\\\nFengyuan Liu1+,\\\\nNouar AlDahoul1+,\\\\nGregory Eady2,\\\\nYasir=
 Zaki1,\\\\\\\\*,\\\\nTalal Rahwan1,\\\\\\\\*\\\\n\\\\n1New York University=
 Abu Dhabi, UAE\\\\n2University of Copenhagen, Denmark\\\\n\\\\n+Joint firs=
t authors.\\\\n\\\\\\\\*Joint senior authors. Correspondence: yasir.zaki@ny=
u.edu, talal.rahwan@nyu.edu\\\\n\\\\n###### Abstract\\\\n\\\\nPrevious stud=
ies proposed that the reasoning capabilities of large language models (LLMs=
) can be improved through self-reflection, i.e., letting LLMs reflect on th=
eir own output to identify and correct mistakes in the initial responses. H=
owever, earlier experiments offer mixed results when it comes to the benefi=
ts of self-reflection. Furthermore, prior studies on self-reflection are pr=
edominantly concerned with the reasoning capabilities of models, ignoring t=
he potential for self-reflection in safety, bias, and ideological leaning.\=
\\\nHere, by conducting a series of experiments testing LLM\\\\u2019s self-=
reflection capability in various tasks using a variety of prompts and diffe=
rent LLMs, we make several contributions to the literature. First, we recon=
cile conflicting findings regarding the benefit of self-reflection, by demo=
nstrating that the outcome of self-reflection is sensitive to prompt wordin=
g\\\\u2014both the original prompt that are used to elicit an initial answe=
r and the subsequent prompt used to self-reflect. Specifically, although se=
lf-reflection may improve the reasoning capability of LLMs when the initial=
 response is simple, the technique cannot improve upon the state-of-the-art=
 chain-of-thought (CoT) prompting. Second, we show that self-reflection can=
 lead to safer (75.8% reduction in toxic responses while preserving 97.8% n=
on-toxic ones), less biased (77% reduction in gender biased responses, whil=
e preserving 94.3% unbiased ones), and more ideologically neutral responses=
 (100% reduction in partisan leaning response, while preserving 87.7% non-p=
artisan ones). The paper concludes by discussing the implications of our fi=
ndings on the deployment of large language models. We release our experimen=
ts at [github.com/Michael98Liu/self-reflection](https://github.com/Michael9=
8Liu/self-reflection).\\\\n\\\\nSelf-Reflection Makes Large Language