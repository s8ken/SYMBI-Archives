 Models=
 Safer, Less Biased, and Ideologically Neutral\\\\n\\\\nFengyuan Liu1+,\\\\=
nNouar AlDahoul1+,\\\\nGregory Eady2,\\\\nYasir Zaki1,\\\\\\\\*,\\\\nTalal =
Rahwan1,\\\\\\\\*\\\\n\\\\n1New York University Abu Dhabi, UAE\\\\n2Univers=
ity of Copenhagen, Denmark\\\\n\\\\n+Joint first authors.\\\\n\\\\\\\\*Join=
t senior authors. Correspondence: yasir.zaki@nyu.edu, talal.rahwan@nyu.edu\=
\\\n\\\\n## 1 Introduction\\\\n\\\\nWhen it comes to the benefits of self-r=
eflection, literature shows conflicting results. Previous research demonstr=
ated that the reasoning capability of large language models (LLMs) can be i=
mproved through self-reflection, i.e., prompting the models to reflect on t=
heir own response to identify and correct potential mistakes, devoid of any=
 external feedback\\\\u00a0Chen et\\\\u00a0al. ([2023](https://arxiv.org/ht=
ml/2406.10400v2#bib.bib2)); Madaan et\\\\u00a0al. ([2024](https://arxiv.org=
/html/2406.10400v2#bib.bib12)).\\\\nHowever, several recent studies have ch=
allenged the benefits of self-reflection by pointing out issues with some o=
f the earlier experiments and providing evidence that self-reflection may a=
ctually deteriorate performance\\\\u00a0Chen et\\\\u00a0al. ([2023](https:/=
/arxiv.org/html/2406.10400v2#bib.bib2)); Huang et\\\\u00a0al. ([2023](https=
://arxiv.org/html/2406.10400v2#bib.bib7)); Stechly et\\\\u00a0al. ([2023](h=
ttps://arxiv.org/html/2406.10400v2#bib.bib18)); Valmeekam et\\\\u00a0al. ([=
2023](https://arxiv.org/html/2406.10400v2#bib.bib19)).\\\\n\\\\nTable\\\\u0=
0a0[1](https://arxiv.org/html/2406.10400v2#S1.T1 \\\\"Table 1 \\\\u2023 1 I=
ntroduction \\\\u2023 Self-Reflection Makes Large Language Models Safer, Le=
ss Biased, and Ideologically Neutral\\\\") summarizes self-reflection liter=
ature, demonstrating two issues with current research on self-reflection. F=
irst, with the exception of [Zhang et\\\\u00a0al.](https://arxiv.org/html/2=
406.10400v2#bib.bib22)\\\\u2019s ([2024](https://arxiv.org/html/2406.10400v=
2#bib.bib22)), studies on self-reflection tend to rely on a single prompt. =
This is potentially problematic, since Huang et\\\\u00a0al. ([2023](https:/=
/arxiv.org/html/2406.10400v2#bib.bib7)) hypothesized that self-reflection p=
rompts might introduce biases steering models away from optimal responses. =
However, the degree to which self-reflection is sensitive to prompt constru=
ction has not been systematically investigated to date. Second, with the so=
le exception of\\\\u00a0[Gou et\\\\u00a0al.](https://arxiv.org/html/2406.10=
400v2#bib.bib5)\\\\u2019s ([2023](https://arxiv.org/html/2406.10400v2#bib.b=
ib5)), the existing literature is predominantly concerned with the reasonin=
g capabilities of LLMs, ignoring other aspects such as safety, fairness, an=
d ideological neutrality that could potentially be improved through self-re=
flection.\\\\n\\\\nAgainst this background, we conduct experiments to syste=
matically investigate how self-reflection affects the reasoning, safety, bi=
as, and ideological leaning of LLMs. To understand the intrinsic ability of=
 such models to self-improve, we specifically focus on self-reflection with=
 natural language prompts, without any external feedback. More precisely, t=
his paper focuses on the following five Research Questions (RQs):\\\\n\\\\n=
* \\\\u2022\\\\n\\\\n  RQ1: How does self-reflection impact reasoning, safe=
ty, gender bias, and ideological leaning?\\\\n* \\\\u2022\\\\n\\\\n  RQ2: H=
ow does the ability to self-reflect vary between proprietary and open-sourc=
e LLMs?\\\\n* \\\\u2022\\\\n\\\\n  RQ3: Is self-reflection sensitive to pro=
mpt construction?\\\\n* \\\\u2022\\\\n\\\\n  RQ4: To what extent does a pro=
mpt that is optimal for a specific LLM and a specific task maintain its eff=
ectiveness across different LLMs and tasks?\\\\n* \\\\u2022\\\\n\\\\n  RQ5:=
 Does cross-reflection between multiple LLMs affect detection of unsafe and=
 ideologically skewed responses?\\\\n\\\\n| Reference | Self-Ref