 \\\\u2023 Self-Reflection Makes Large Lang=
uage Models Safer, Less Biased, and Ideologically Neutral\\\\") for prompts=
 used in experiments.\\\\nFor prompt construction sensitivity, we can see t=
hat different prompts yield varying accuracy results. These findings demons=
trate the sensitivity of LLMs to prompt construction when it comes to ident=
ifying moderate and partisan responses.\\\\n\\\\n ![Refer to caption](https=
://arxiv.org/x4.png)\\\\n\\\\nFigure 4: TPR, TNR, and overall accuracy afte=
r self-reflection using the ideological leaning dataset. The temperature va=
lue is set to 1 for text generation. See Appendix Figure\\\\u00a0[10](https=
://arxiv.org/html/2406.10400v2#A1.F10 \\\\"Figure 10 \\\\u2023 Appendix A A=
ppendix \\\\u2023 Self-Reflection Makes Large Language Models Safer, Less B=
iased, and Ideologically Neutral\\\\") for each of the four prompts used in=
 experiments.\\\\n\\\\n### 4.5 Cross-Reflection\\\\n\\\\nIn the fifth exper=
iment, we implemented a cross-reflection technique, where one LLM evaluates=
, critiques, or refines the responses generated by another LLM. We applied =
this technique to two datasets: a safety dataset and an ideological leaning=
 dataset. Given its superior performance in the self-reflection scenario, w=
e selected GPT to critique the responses of Gemini and Llama. GPT effective=
ly detected unsafe user queries and improved the safety of Gemini and Llama=
, achieving a high accuracy of 79.51%. Simultaneously, it preserved the mod=
els\\\\u2019 helpfulness with an accuracy of 82.09%. Additionally, it ident=
ified partisan-aligned responses and enhanced ideological neutrality with a=
n impressive accuracy of 99.48%, while still recognizing moderate responses=
 with an accuracy of 65.83%. For consistency, we used the same prompts from=
 the safety and ideological leaning experiments. The Table\\\\u00a0[9](http=
s://arxiv.org/html/2406.10400v2#A1.T9 \\\\"Table 9 \\\\u2023 Appendix A App=
endix \\\\u2023 Self-Reflection Makes Large Language Models Safer, Less Bia=
sed, and Ideologically Neutral\\\\") and [10](https://arxiv.org/html/2406.1=
0400v2#A1.T10 \\\\"Table 10 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Re=
flection Makes Large Language Models Safer, Less Biased, and Ideologically =
Neutral\\\\") in Appendix presents the TPR, TNR, and overall accuracy of th=
e three LLMs evaluated on the seven (for safety) and four prompts (for ideo=
logical leaning).\\\\n\\\\n### 4.6 Summary of Evaluations\\\\n\\\\nTo sum u=
p, although self-reflection only marginally improves the reasoning ability =
of LLMs, it can significantly increases the safety, reduces the bias, and d=
ecrease partisan leaning of LLMs (RQ1). Furthremore, proprietary LLMs (name=
ly, GPT-4o-mini and Gemini 1.5-Flash) consistently outperform the open-sour=
ce LLM (i.e., Llama 3.2-3B) in our experiments (RQ2). However, this could b=
e due to both GPT-4o-mini and Gemini 1.5-Flash having more parameters compa=
red to Llama 3.2-3B. Additionally, all three LLMs tested in our experiments=
 are sensitive to prompt construction (RQ3), and different models have diff=
erent optimal self-reflection prompt (RQ4); a prompt optimal for one model =
could be the worst prompt for another model. Finally, although using GPT to=
 cross-reflect the responses of Gemini and Llama yields better performance =
than Gemini and Llama alone, GPT provides the highest accuracy when self-re=
flecting its own output (RQ5).\\\\nWe supplement Figures\\\\u00a0[1](https:=
//arxiv.org/html/2406.10400v2#S4.F1 \\\\"Figure 1 \\\\u2023 4.1 Self-Reflec=
tion Marginally Improves Reasoning Capability \\\\u2023 4 Evaluations \\\\u=
2023 Self-Reflection Makes Large Language Models Safer, Less Biased, and Id=
eologically Neutral\\\\") to [4](https://arxiv.org/html/2406.10400v2#S4.F4 =
\\\\"Figure 4 \\\\u2023 4.4 Self-Reflection Improves Partisan Neutrality \\=
\\u2023 4 Evaluations \\\\u2023 Self-Reflection Makes Large Language Models=
 Safer, Less Biased, and Ide