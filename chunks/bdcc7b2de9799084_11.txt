ologically Neutral\\\\") by reporting the full =
numerical results in Appendix Table\\\\u00a0[2](https://arxiv.org/html/2406=
.10400v2#A1.T2 \\\\"Table 2 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Re=
flection Makes Large Language Models Safer, Less Biased, and Ideologically =
Neutral\\\\") to [8](https://arxiv.org/html/2406.10400v2#A1.T8 \\\\"Table 8=
 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Reflection Makes Large Langua=
ge Models Safer, Less Biased, and Ideologically Neutral\\\\")). Furthermore=
, as a robustness check, all experiments are repeated for temperature=3D0, =
resulting in qualitatively similar results (see Table\\\\u00a0[11](https://=
arxiv.org/html/2406.10400v2#A1.T11 \\\\"Table 11 \\\\u2023 Appendix A Appen=
dix \\\\u2023 Self-Reflection Makes Large Language Models Safer, Less Biase=
d, and Ideologically Neutral\\\\") to [16](https://arxiv.org/html/2406.1040=
0v2#A1.T16 \\\\"Table 16 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Refle=
ction Makes Large Language Models Safer, Less Biased, and Ideologically Neu=
tral\\\\") for results).\\\\n\\\\n## 5 Discussion and Conclusion\\\\n\\\\nI=
n this study, we showed that while self-reflection shows a limited effect o=
n improving the reasoning capabilities of language models, it is capable of=
 drastically improving the safety, gender neutrality, and political neutral=
ity of language models. Given that most existing research on self-reflectio=
n focuses on reasoning ability, our findings point to new avenues of resear=
ch on the underexplored areas where self-reflection is beneficial.\\\\n\\\\=
nWe theorize that self-reflection is best used when an LLM needs to meet mu=
ltiple objectives at the same time, e.g., when a language model is asked to=
 translate a sentence while, at the same time, ensuring that the output is =
not toxic. Our experiments demonstrated that LLMs tend to be bad at such mu=
lti-tasking. In particular, when asked to translate, it occasionally does s=
o while disregarding safety issues. To put it differently, the first task (=
the translation) occasionally distracts the model from the second task (ens=
uring that the output is not toxic). Our analysis suggests that such multi-=
tasking scenarios are where self-reflection can significantly improve model=
 performance.\\\\n\\\\nIn conclusion, we propose self-reflection as a low-c=
ost measure that can be deployed by any language model at test time to impr=
ove safety, gender neutrality, and political neutrality. Our work contribut=
es to the growing body of research exploring the test-time scaling idea\\\\=
u00a0Muennighoff et\\\\u00a0al. ([2025](https://arxiv.org/html/2406.10400v2=
#bib.bib13)), where models are allocated extra test-time compute to improve=
 performance.\\\\n\\\\n## Limitations\\\\n\\\\nWe conduct all self-reflecti=
on experiments using the small versions of three LLMs, namely GPT-4o-mini, =
Gemini-1.5-Flash, and Llama-3.2-3B. Therefore, it remains to be demonstrate=
d whether bigger models exhibit the same behavior. Additionally, although o=
ur experiments demonstrated that the outcome of self-reflection is sensitiv=
e to prompt construction, it remains a challenge to identify the most optim=
al prompt. Future work is needed to come up with systematic ways to constru=
ct the most optimal prompt used for self-reflection given a specific task a=
nd model.\\\\n\\\\n## References\\\\n\\\\n* Arakelyan et\\\\u00a0al. (2024)=
\\\\n\\\\n  Erik Arakelyan, Zhaoqi Liu, and Isabelle Augenstein. 2024.\\\\n=
  Semantic sensitivities and inconsistent predictions: Measuring the fragil=
ity of nli models.\\\\n  *arXiv preprint arXiv:2401.14440*.\\\\n* Chen et\\=
\\u00a0al. (2023)\\\\n\\\\n  Xinyun Chen, Maxwell Lin, Nathanael Sch\\\\u00=
e4rli, and Denny Zhou. 2023.\\\\n  Teaching large language models to self-d=
ebug.\\\\n  *arXiv preprint arXiv:2304.05128*.\\\\n* Cobbe et\\\\u00a0al. (=
2021)\\\\n\\\\n  Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,=
 Heewoo Jun, Lukasz Kais