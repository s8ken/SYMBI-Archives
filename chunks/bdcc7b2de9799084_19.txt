 L=
i, and Yu\\\\u00a0Li. 2023.\\\\n  Progressive-hint prompting improves reaso=
ning in large language models.\\\\n  *arXiv preprint arXiv:2304.09797*.\\\\=
n\\\\n## Appendix A Appendix\\\\n\\\\n|  | Simple Original Prompt | | | CoT=
 Original Prompt | | |\\\\n| --- | --- | --- | --- | --- | --- | --- |\\\\n=
| Prompt | Gemini | GPT | Llama | Gemini | GPT | Llama |\\\\n|  | 55.6% | 6=
2.1% | 36.1% | 60.1% | 65.5% | 39.1% |\\\\n| Original answer | (0.275) | (0=
.432) | (0.806) | (0.198) | (0.212) | (0.489) |\\\\n|  | 51.2% | 55.1% | 30=
.9% | 58.4% | 65.7% | 34.0% |\\\\n| Prompt 1 | (0.276) | (0.515) | (1.064) =
| (6.328) | (0.091) | (0.915) |\\\\n|  | 45.8% | 52.5% | 29.8% | 58.0% | 65=
.5% | 33.5% |\\\\n| Prompt 2 | (3.455) | (0.921) | (1.259) | (6.803) | (0.0=
79) | (1.896) |\\\\n|  | 49.4% | 59.2% | 29.2% | 58.3% | 65.6% | 33.8% |\\\=
\n| Prompt 3 | (1.133) | (0.091) | (1.100) | (6.509) | (0.236) | (0.371) |\=
\\\n|  | 52.1% | 59.8% | 30.7% | 59.7% | 65.0% | 34.8% |\\\\n| Prompt 4 | (=
3.554) | (0.569) | (0.971) | (3.717) | (0.518) | (1.289) |\\\\n|  | 43.4% |=
 63.7% | 33.3% | 57.8% | 65.1% | 37.1% |\\\\n| Prompt 5 | (9.296) | (0.863)=
 | (1.259) | (3.179) | (0.820) | (0.811) |\\\\n\\\\nTable 2: Self-reflectio=
n experiments using MEDQA-USMLE. The temperature value is set to 1 for text=
 generation. To obtain the original answer, we prompted each LLM in two dif=
ferent ways: The first uses a simple prompt without chain-of-thought (CoT) =
prompting, while the second uses a more complex prompt with CoT prompting. =
For each original answer, we experimented with five different variations of=
 self-reflection prompts. We repeat all experiments three times and report =
the average accuracy and its standard deviations (in brackets) across all r=
epetitions. See the Appendix Figure\\\\u00a0[6](https://arxiv.org/html/2406=
.10400v2#A1.F6 \\\\"Figure 6 \\\\u2023 Appendix A Appendix \\\\u2023 Self-R=
eflection Makes Large Language Models Safer, Less Biased, and Ideologically=
 Neutral\\\\") for the exact prompts used in experiments.\\\\n\\\\n|  | Sim=
ple Original Prompt | | | CoT Original Prompt | | |\\\\n| --- | --- | --- |=
 --- | --- | --- | --- |\\\\n| Prompt | Gemini | GPT | Llama | Gemini | GPT=
 | Llama |\\\\n|  | 37.3% | 92.6% | 59.8% | 93.1% | 92.0% | 62.8% |\\\\n| O=
riginal answer | (0.493) | (0.463) | (2.487) | (0.227) | (0.461) | (1.082) =
|\\\\n|  | 24.1% | 92.3% | 44.4% | 63.6% | 90.9% | 52.6% |\\\\n| Prompt 1 |=
 (1.097) | (0.457) | (1.756) | (0.888) | (0.548) | (0.570) |\\\\n|  | 27.9%=
 | 92.3% | 38.3% | 26.6% | 91.4% | 47.4% |\\\\n| Prompt 2 | (0.662) | (0.54=
7) | (1.919) | (0.631) | (0.374) | (0.289) |\\\\n|  | 7.1% | 90.5% | 39.5% =
| 18.0% | 88.7% | 48.3% |\\\\n| Prompt 3 | (0.463) | (0.418) | (0.526) | (0=
.919) | (0.444) | (2.220) |\\\\n|  | 82.5% | 60.5% | 46.3% | 92.7% | 50.5% =
| 47.5% |\\\\n| Prompt 4 | (0.717) | (2.622) | (0.871) | (0.116) | (0.497) =
| (0.809) |\\\\n|  | 88.1% | 92.0% | 49.5% | 92.5% | 91.6% | 54.3% |\\\\n| =
Prompt 5 | (0.862) | (0.347) | (2.437) | (0.579) | (0.244) | (0.720) |\\\\n=
|  | 42.9% | 87.2% | 50.5% | 44.8% | 83.1% | 55.6% |\\\\n| Prompt 6 | (1.40=
3) | (0.613) | (1.579) | (1.253) | (1.032) | (0.616) |\\\\n|  | 21.2% | 83.=
3% | 46.6% | 22.4% | 78.5% | 52.5% |\\\\n| Prompt 7 | (0.688) | (0.648) | (=
2.164) | (0.131) | (0.152) | (0.674) |\\\\n|  | 17.4% | 90.1% | 44.4% | 10.=
0% | 87.2% | 49.5% |\\\\n| Prompt 8 | (1.051) | (0.244) | (2.752) | (0.244)=
 | (0.759) | (1.134) |\\\\n|  | 1.7% | 85.8% | 15.8% | 2.5% | 78.9% | 17.3%=
 |\\\\n| Prompt 9 | (0.358) | (0.374) | (1.022) | (0.088) | (0.306) | (0.93=
1) |\\\\n\\\\nTable 3: Self-reflection experiments using GSM8K. The experim=
ents are conducted in the same way as in Table\\\\u00a0[2](https://arxiv.or=
g/html/2406.10400v2#A1.T2 \\\\"Table 2 \\\\u2023 Appendix A Appendix \\\\u2=
023 Self-Reflection Makes Large Language Models Safer, Less Biased, and Ide=
ologically Neutral\\\\") but with four additional prompts. See Append