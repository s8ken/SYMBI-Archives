 We experi=
mented with seven different variations of self-reflection prompts. We repea=
t all experiments three times and report the average accuracy and its stand=
ard deviations (in brackets) across all repetitions. See the appendix for t=
he exact prompts used in experiments.\\\\n\\\\n|  | True Positive Rate | | =
| True Negative Rate | | | Accuracy | | |\\\\n| --- | --- | --- | --- | ---=
 | --- | --- | --- | --- | --- |\\\\n| Prompt | Gemini | GPT | Llama | Gemi=
ni | GPT | Llama | Gemini | GPT | Llama |\\\\n|  | 0% | 0% | 0% | 100% | 10=
0% | 100% |  |  |  |\\\\n| Original answer | (0) | (0) | (0) | (0) | (0) | =
(0) | 52% | 52% | 52% |\\\\n|  | 32.4% | 41.1% | 22.6% | 100.0% | 100.0% | =
82.9% |  |  |  |\\\\n| Prompt 1 | (0.280) | (0.318) | (0.462) | (0.000) | (=
0.000) | (1.089) | 67.6% | 71.7% | 54.0% |\\\\n|  | 31.7% | 42.6% | 30.6% |=
 100.0% | 100.0% | 45.0% |  |  |  |\\\\n| Prompt 2 | (0.183) | (0.280) | (0=
.662) | (0.000) | (0.000) | (1.793) | 67.2% | 72.5% | 38.1% |\\\\n|  | 38.3=
% | 43.1% | 31.8% | 100.0% | 100.0% | 57.2% |  |  |  |\\\\n| Prompt 3 | (0.=
280) | (0.106) | (0.328) | (0.000) | (0.000) | (0.801) | 70.4% | 72.7% | 45=
.0% |\\\\n|  | 31.8% | 59.0% | 70.0% | 100.0% | 99.6% | 68.6% |  |  |  |\\\=
\n| Prompt 4 | (0.106) | (1.734) | (0.280) | (0.000) | (0.293) | (0.705) | =
67.2% | 80.1% | 69.3% |\\\\n|  | 35.1% | 58.4% | 76.1% | 100.0% | 99.9% | 3=
3.6% |  |  |  |\\\\n| Prompt 5 | (0.183) | (0.183) | (1.539) | (0.000) | (0=
.098) | (1.477) | 68.8% | 80.0% | 54.0% |\\\\n|  | 36.9% | 54.2% | 27.0% | =
100.0% | 100.0% | 96.4% |  |  |  |\\\\n| Prompt 6 | (0.382) | (0.530) | (1.=
121) | (0.000) | (0.000) | (0.685) | 69.7% | 78.0% | 63.1% |\\\\n|  | 44.5%=
 | 75.3% | 55.2% | 100.0% | 82.9% | 12.2% |  |  |  |\\\\n| Prompt 7 | (0.18=
3) | (0.485) | (1.043) | (0.000) | (3.522) | (0.402) | 73.4% | 79.3% | 32.8=
% |\\\\n\\\\nTable 6: No-reflection experiments using the safety dataset. \=
\\\u201cNo-reflection\\\\u201d means that LLMs are asked to pay attention t=
o safety issues while translating, instead of being asked to self-reflect a=
fter translation, using the same prompts used in self-reflection experiment=
s as in reported in Table\\\\u00a0[5](https://arxiv.org/html/2406.10400v2#A=
1.T5 \\\\"Table 5 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Reflection M=
akes Large Language Models Safer, Less Biased, and Ideologically Neutral\\\=
\"). The temperature value is set to 1 for text generation. We experimented=
 with seven different variations of no-reflection prompts. We repeat all ex=
periments three times and report the average accuracy and its standard devi=
ations (in brackets) across all repetitions. See the appendix for the exact=
 prompts used in experiments.\\\\n\\\\n|  | True Positive Rate | | | True N=
egative Rate | | | Accuracy | | |\\\\n| --- | --- | --- | --- | --- | --- |=
 --- | --- | --- | --- |\\\\n| Prompt | Gemini | GPT | Llama | Gemini | GPT=
 | Llama | Gemini | GPT | Llama |\\\\n|  | 0% | 0% | 0% | 100% | 100% | 100=
% |  |  |  |\\\\n| Original answer | (0) | (0) | (0) | (0) | (0) | (0) | 50=
% | 50% | 50% |\\\\n|  | 24.7% | 77.0% | 34.2% | 99.9% | 94.3% | 50.5% |  |=
  |  |\\\\n| Prompt 1 | (0.249) | (0.492) | (0.340) | (0.082) | (0.498) | (=
1.173) | 62.3% | 85.6% | 42.3% |\\\\n|  | 44.2% | 46.6% | 38.3% | 99.4% | 9=
8.9% | 48.8% |  |  |  |\\\\n| Prompt 2 | (0.125) | (0.624) | (1.682) | (0.2=
45) | (0.125) | (1.563) | 71.8% | 72.7% | 43.6% |\\\\n|  | 34.0% | 50.4% | =
34.9% | 99.9% | 98.5% | 54.0% |  |  |  |\\\\n| Prompt 3 | (0.638) | (0.883)=
 | (0.287) | (0.047) | (0.094) | (1.651) | 67.0% | 74.4% | 44.5% |\\\\n|  |=
 24.7% | 9.6% | 17.8% | 99.6% | 99.9% | 88.9% |  |  |  |\\\\n| Prompt 4 | (=
0.464) | (0.205) | (0.634) | (0.082) | (0.047) | (1.021) | 62.1% | 54.7% | =
53.3% |\\\\n\\\\nTable 7: Self-reflection experiments using the gender bias=
 dataset. The temperature value is set to 1 for text generation. We experim=
ented with four different variations of self-reflection promp