ch has been sho=
wn to improve the performance of LLMs in various tasks\\\\u00a0Chen et\\\\u=
00a0al. ([2023](https://arxiv.org/html/2406.10400v2#bib.bib2)).\\\\n\\\\nHo=
wever, more recent studies challenged these claims by presenting mixed resu=
lts when it comes to the benefit of self-reflection\\\\u00a0Li et\\\\u00a0a=
l. ([2024](https://arxiv.org/html/2406.10400v2#bib.bib10)); Huang et\\\\u00=
a0al. ([2023](https://arxiv.org/html/2406.10400v2#bib.bib7)).\\\\nNotably, =
Huang et\\\\u00a0al. ([2023](https://arxiv.org/html/2406.10400v2#bib.bib7))=
 pointed out three major issues with some earlier experiments and concluded=
 that the observed performance gain should not be attributed to self-reflec=
tion but rather to various exogenous factors. In our study, we avoid the sa=
me mistakes documented in\\\\u00a0[Huang et\\\\u00a0al.](https://arxiv.org/=
html/2406.10400v2#bib.bib7)\\\\u2019s ([2023](https://arxiv.org/html/2406.1=
0400v2#bib.bib7)), e.g., by refraining from providing additional informatio=
n (such as oracle labels) in the self-reflection prompt.\\\\n\\\\nLLMs are =
sensitive to prompt construction. Studies have shown that prompt constructi=
on is crucial to elicit the reasoning ability of LLMs\\\\u00a0Wei et\\\\u00=
a0al. ([2022](https://arxiv.org/html/2406.10400v2#bib.bib21)). Meanwhile, a=
dversarially constructed prompts could hurt performance by simply changing =
a few words in the prompt\\\\u00a0Arakelyan et\\\\u00a0al. ([2024](https://=
arxiv.org/html/2406.10400v2#bib.bib1)); Verma et\\\\u00a0al. ([2024](https:=
//arxiv.org/html/2406.10400v2#bib.bib20)). Our study contributes to this li=
ne of research by documenting a specific case of sensitivity introduced by =
prompts in the context of self-reflection.\\\\n\\\\n## 3 Data and Experimen=
ts\\\\n\\\\n### 3.1 Reasoning Datasets\\\\n\\\\nTo assess whether self-refl=
ection improves the reasoning capabilities of LLMs, we use three datasets. =
The first is the MEDQA-USMLE dataset consisting of questions and\\\\ntheir =
associated answer from a professional medical exam, namely the Medical Boar=
d Examination in the USA\\\\u00a0Jin et\\\\u00a0al. ([2021](https://arxiv.o=
rg/html/2406.10400v2#bib.bib9)). These questions are designed to examine th=
e doctors\\\\u2019 professional knowledge, which means that many questions =
require multi-hop logical reasoning.\\\\nThe second is the Massive Multitas=
k Language Understanding (MMLU), a benchmark dataset consisting of multiple=
-choice questions drawn from 57 academic subjects across STEM, the humaniti=
es, and the social sciences\\\\u00a0Hendrycks et\\\\u00a0al. ([2020](https:=
//arxiv.org/html/2406.10400v2#bib.bib6)). The second is the GSM8K, a large =
dataset of grade school math problems\\\\u00a0Cobbe et\\\\u00a0al. ([2021](=
https://arxiv.org/html/2406.10400v2#bib.bib3)). Our experiments uses the te=
st set of MEDQA-USMLE (1273 questions), MMLU (a random sample of 1,000 ques=
tions), and GSM8K (1319 questions).\\\\n\\\\n### 3.2 Safety Dataset\\\\n\\\=
\nTo assess whether self-reflection improves the safety guardrails of LLMs,=
 we use a dataset of French sentences by combining a dataset of safe and an=
other dataset of unsafe (i.e., those with offensive language) French senten=
ces. The unsafe and safe examples are a randomly selected subset of the saf=
ety dataset proposed by [Jan et\\\\u00a0al.](https://arxiv.org/html/2406.10=
400v2#bib.bib8)\\\\u2019s ([2025](https://arxiv.org/html/2406.10400v2#bib.b=
ib8)).\\\\n\\\\nTo curate the final dataset of a mixture of safe and unsafe=
 content, we first attempted to translate each French sentence from those t=
wo datasets to English using three LLMs including GPT-4o-mini\\\\u00a0OpenA=
I et\\\\u00a0al. ([2024](https://arxiv.org/html/2406.10400v2#bib.bib14)), G=
emini-1.5-Flash\\\\u00a0GeminiTeam et\\\\u00a0al. ([2024](https://arxiv.org=
/html/2406.10400v2#bib.bib4)), and Llama-3.2-3B\\\\u00a0Llama\\\\u00a0Team =
([2024](https://arxiv.org/html/2406.10400v2#bib.bib11)).