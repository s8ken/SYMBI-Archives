 Then, for each res=
ponse provided by an LLM, we used GPT-4o Judge\\\\u00a0Jan et\\\\u00a0al. (=
[2025](https://arxiv.org/html/2406.10400v2#bib.bib8)) to determine whether =
the LLM provides a translation or not.\\\\nIdeally, the LLMs should transla=
te every safe sentence, and should refuse to translate every unsafe sentenc=
e. However, we found 445 unsafe sentences for which all three LLMs provided=
 a translation (instead of refusing to translate). We also found 482 safe s=
entences for which all three LLMs provided a translation (as they should).\=
\\\n\\\\nOur final dataset consists of all the aforementioned 927 sentence-=
translation pairs. As such, without any self-reflection, 48% of the respons=
es (445) were unsafe, while the remaining 52% of responses (482) were safe.=
 See Appendix Figure\\\\u00a0[11](https://arxiv.org/html/2406.10400v2#A1.F1=
1 \\\\"Figure 11 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Reflection Ma=
kes Large Language Models Safer, Less Biased, and Ideologically Neutral\\\\=
") and [12](https://arxiv.org/html/2406.10400v2#A1.F12 \\\\"Figure 12 \\\\u=
2023 Appendix A Appendix \\\\u2023 Self-Reflection Makes Large Language Mod=
els Safer, Less Biased, and Ideologically Neutral\\\\") for examples in our=
 dataset. Later on in our experiments, we apply self-reflection to determin=
e whether it affects the percentage of safe responses.\\\\n\\\\n### 3.3 Gen=
der Bias Dataset\\\\n\\\\nTo evaluate whether self-reflection improves the =
bias guardrails of LLMs, we create a benchmark dataset for gender bias, con=
sisting of biased and unbiased responses generated by the above three LLMs.=
 Here, we consider an LLM response to be gender biased if it needlessly use=
s a gendered pronoun in response to a generic question that did not require=
 the use of such a pronoun. In contrast, unbiased responses are those that =
do not use any pronouns or use gender-neutral pronouns such as \\\\u201cthe=
y\\\\u201d or \\\\u201cthem.\\\\u201d\\\\n\\\\nTo create this dataset, we f=
irst asked each LLM to generate 3000 responses given the following prompt: =
Discuss a person practicing a profession in 10 lines. We randomly selected =
1000 responses that contain gender pronouns (i.e., she, he, her, his, and h=
im), and labeled them as biased. We also randomly selected 700 responses th=
at do not contain such pronouns and labeled them as unbiased.\\\\n\\\\nIt s=
hould be noted that not all gender pronouns are associated with bias. To en=
sure that our dataset is not creating a spurious correlation between gender=
 bias and gender pronoun, we supplement the unbiased dataset with sentences=
 that contain a gender pronoun yet do not have gender bias. To this end, we=
 prompt each language model 100 times to generate sentences discussing the =
impact of a specific public figure. For example, the prompts include Discus=
s Nelson Mandela\\\\u2019s leadership and its impact on South Africa, Discu=
ss Thomas Edison\\\\u2019s role in shaping modern electrical technology, an=
d Discuss Jackie Robinson\\\\u2019s breaking of baseball\\\\u2019s color ba=
rrier. All these prompts are themselves generated by an LLM. These 300 resp=
onses contain gender pronouns when discussing a certain profession (e.g., p=
oliticians, athletes, etc.), yet they are not biased because they are conce=
rned with specific people with publicly known genders. Together, this gives=
 us a balanced dataset of 50% of responses (1000) with gender bias and 50% =
of responses (700 + 300) without such bias. See Appendix Figure\\\\u00a0[13=
](https://arxiv.org/html/2406.10400v2#A1.F13 \\\\"Figure 13 \\\\u2023 Appen=
dix A Appendix \\\\u2023 Self-Reflection Makes Large Language Models Safer,=
 Less Biased, and Ideologically Neutral\\\\") and [14](https://arxiv.org/ht=
ml/2406.10400v2#A1.F14 \\\\"Figure 14 \\\\u2023 Appendix A Appendix \\\\u20=
23 Self-Reflection Makes Large Language Models Safer, Less Biased, and Ideo=
logically Neutral\\\\") for examples in our dataset.