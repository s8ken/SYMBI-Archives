valuations \\\\u2023 Self-R=
eflection Makes Large Language Models Safer, Less Biased, and Ideologically=
 Neutral\\\\"), the effectiveness of self-reflection varies significantly d=
epending on both the reflection prompt and the language model being evaluat=
ed. For example, when asked to self-reflect using the prompt \\\\u201ccriti=
cally reflect upon original answer\\\\u201d, GPT achieves an accuracy of 63=
.7% on answering medical questions, achieving higher accuracy than 62.1%, i=
ts original accuracy. However, the accuracy is much lower (52.5%) if self-r=
eflection is done by asking the model to verify if \\\\u201coriginal answer=
 is wrong\\\\u201d. Even the same prompt can lead to dramatically different=
 performance with different language models. For example, although the prom=
pt \\\\u201ccritically reflect upon original answer\\\\u201d results in hig=
her accuracy in GPT, it leads much lower accuracy with Gemini (55.6% to 43.=
4%). This suggests that any researchers evaluating the impact of self-refle=
ction should do so with a variety of prompts and language models. See Appen=
dix Figure\\\\u00a0[6](https://arxiv.org/html/2406.10400v2#A1.F6 \\\\"Figur=
e 6 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Reflection Makes Large Lan=
guage Models Safer, Less Biased, and Ideologically Neutral\\\\") and [7](ht=
tps://arxiv.org/html/2406.10400v2#A1.F7 \\\\"Figure 7 \\\\u2023 Appendix A =
Appendix \\\\u2023 Self-Reflection Makes Large Language Models Safer, Less =
Biased, and Ideologically Neutral\\\\") for a full list of the exact prompt=
s tested.\\\\n\\\\nSecond, our experiments reveal that the reasoning abilit=
y of language models does not improve, or only marginally improve, after se=
lf-reflection. Specifically, when answering medical questions in the MEDQA-=
USMLE dataset and the math problems in the GSM8K dataset, language models a=
chieve the best accuracy when prompted using CoT without self-reflection. W=
hen answering multiple-choice questions in the MMLU dataset, language model=
s can achieve marginally better accuracy with self-reflection, but the diff=
erences are statistically insignificant in all cases.\\\\nWe do notice some=
 exceptions, however, where self-reflection improves upon the original accu=
racy. When Gemini is initially asked to solve math problems without outputt=
ing the intermediate reasoning steps, for example, it originally achieved a=
n accuracy of 37.3% (without intermediate reasoning steps), but the accurac=
y rose to 88.1% when the model was prompted to critically reflect on its or=
iginal answer (Appendix Figure\\\\u00a0[3](https://arxiv.org/html/2406.1040=
0v2#A1.T3 \\\\"Table 3 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Reflect=
ion Makes Large Language Models Safer, Less Biased, and Ideologically Neutr=
al\\\\")). However, this improvement remains lower than the accuracy achiev=
ed using CoT prompting alone (93.1%). Note that both GPT and Llama provide =
intermediate reasoning steps even when they are not explicitly instructed t=
o do so. Hence, they do not benefit from self-reflection the same way Gemin=
i does.\\\\n\\\\nIntuitively, for self-reflection to be effective, LLMs mus=
t be able to identify and correct its mistakes while preserving its already=
-correct answers. Therefore, we hypothesize that the reason why self-reflec=
tion often fails to improve accuracy is that the models fail to distinguish=
 between correct and incorrect initial answers during self-reflection. To t=
est this hypothesis, we use GSM8K as an example and plot the percentage of =
answers that are changed during self-reflection among answers that were ini=
tially wrong, and those that were initially correct, respectively. We find =
that the rates at which correct and incorrect answers are modified are stro=
ngly correlated (Appendix Figure\\\\u00a0[5](https://arxiv.org/html/2406.10=
400v2#A1.F5 \\\\"Figure 5 \\\\u2023 Appendix A Appendix \\\\u2023 Self-Refl=
ection Makes Large Language Models Safer, Le