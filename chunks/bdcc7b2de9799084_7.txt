ss Biased, and Ideologically Ne=
utral\\\\")). This indicates that language models tend to either change bot=
h correct or wrong answers, or retain both, at a high rate, demonstrating a=
n inability to reliably distinguish between correct and incorrect responses=
.\\\\n\\\\n ![Refer to caption](https://arxiv.org/x1.png)\\\\n\\\\nFigure 1=
: The accuracy on MEDQA-USMLE dataset before and after self-reflection. The=
 accuracy before self-reflection is denoted as \\\\u201coriginal answer\\\\=
u201d, and the rest correspond to accuracies after self-reflection using on=
e of the five prompts (Appendix Figure\\\\u00a0[6](https://arxiv.org/html/2=
406.10400v2#A1.F6 \\\\"Figure 6 \\\\u2023 Appendix A Appendix \\\\u2023 Sel=
f-Reflection Makes Large Language Models Safer, Less Biased, and Ideologica=
lly Neutral\\\\")). Panel on the left correspond to self-reflections on ini=
tial responses obtained using simple initial response without CoT, while th=
e panel on the right correspond to self-reflection on initial responses wit=
h CoT. The temperature value is set to 1 for text generation.\\\\n\\\\n### =
4.2 Self-Reflection Improves Safety\\\\n\\\\nHaving demonstrated that self-=
reflection only marginally improves the reasoning capability of language mo=
dels, we next turn to evaluate the ability of self-reflection to trigger th=
e safety guardrails in LLMs. In this context, the primary goal of self-refl=
ection is to refuse translating sentences with harmful content while retain=
ing the translation of the safe ones. To see whether such is the case, we u=
sed two performance metrics. First, we evaluated whether LLMs can better de=
tect unsafe responses after self-reflection; this was done by calculating t=
he percentage of unsafe responses that are correctly blocked (i.e., true po=
sitive rate, or TPR). Second, we assessed the models\\\\u2019 helpfulness (=
overblocking), measured as the number of safe responses retained (true nega=
tive rate, or TNR). As can be seen in Figure\\\\u00a0[2](https://arxiv.org/=
html/2406.10400v2#S4.F2 \\\\"Figure 2 \\\\u2023 4.2 Self-Reflection Improve=
s Safety \\\\u2023 4 Evaluations \\\\u2023 Self-Reflection Makes Large Lang=
uage Models Safer, Less Biased, and Ideologically Neutral\\\\"), we found t=
hat different LLMs exhibit different levels of safety in their responses af=
ter self-reflection, likely due to differences in their architectures, trai=
ning data, alignment processes, and optimization goals. Specifically, GPT-4=
o achieves the best overall accuracy of 86.8%, outperforming both Gemini (6=
8.2%) and Llama (56.9%) after self-reflection. GPT-4o was able to detect un=
safe users\\\\u2019 queries and enhance the model\\\\u2019s safety with a h=
igh TPR of 75.8%. Additionally, it is still able to keep the model\\\\u2019=
s helpfulness with a TNR of 97.7%. This shows the excellent self-reflection=
 capability of GPT-4o.\\\\nWhile Gemini was only able to enhance safety gua=
rdrail by 36.3%, it does so without compromising the helpfulness. Llama, ho=
wever, degraded the helpfulness without enhancing the safety significantly.=
\\\\n\\\\nOverall, GPT is well-balanced and has a stronger self-awareness f=
ilter, preventing it from confidently engaging in potentially harmful topic=
s. On the other hand, Gemini\\\\u2019s design choices may stem from a prefe=
rence for usability (correct translation) over strict filtering, leading to=
 strong self-reflection in safe contexts but weaker self-awareness in handl=
ing unsafe cases.\\\\n\\\\nSo far, we have demonstrated that both GPT and G=
emini improve their ability to distinguish between safe and unsafe response=
s when prompted to self-reflect upon their own responses. However, it is no=
t entirely clear whether such improvements are due to self-reflection, or s=
imply due to the extra words contained in the self-reflection prompts.\\\\n=
In other words, we ask: Can a language model just as accurately distinguish=
 between safe and unsafe responses if as