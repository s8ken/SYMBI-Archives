 the model\\\\u2019s fairness with a hi=
gh TPR of 77.0%. Additionally, it is still able to detect unbiased response=
s with a TNR of 94.3%. These results demonstrate the remarkable self-reflec=
tion capability of GPT-4o. On the other hand, both Gemini and Llama exhibit=
 worse performance compared to GPT. However, Gemini was able to reduce gend=
er bias by 44.2% while keeping false positive rate low (0.6% FPR, or 99.4% =
TNR). Llama increased the false positives without substantially improving t=
he bias detection. Finally, the specific prompts used to elicit self-reflec=
tions affect the language models\\\\u2019 ability to accurately distinguish=
 biased responses from unbiased ones. See Appendix Figure\\\\u00a0[9](https=
://arxiv.org/html/2406.10400v2#A1.F9 \\\\"Figure 9 \\\\u2023 Appendix A App=
endix \\\\u2023 Self-Reflection Makes Large Language Models Safer, Less Bia=
sed, and Ideologically Neutral\\\\") for prompts used.\\\\n\\\\nWe notice s=
everal issues limit LLMs\\\\u2019 ability to further reduce gender bias whe=
n self-reflecting. GPT has classified certain biased responses as unbiased =
due to its default association of professional roles with male figures when=
 the pronoun \\\\u201che\\\\u201d is used. Furthermore, GPT also perceives =
responses as unbiased when the pronoun \\\\u201cshe\\\\u201d is included.Si=
milarly, Gemini defended labeling its gender-biased response as unbiased by=
 arguing that language mirrors reality. It claimed that referring to a \\\\=
u201cflight attendant\\\\u201d as \\\\u201cshe\\\\u201d is reasonable and t=
hat using \\\\u201che\\\\u201d for a \\\\u201cplumber\\\\u201d is grammatic=
ally correct. Additionally, it stated a preference for avoiding awkward phr=
asing, such as using \\\\u201cthey.\\\\u201d\\\\n\\\\n ![Refer to caption](=
https://arxiv.org/x3.png)\\\\n\\\\nFigure 3: TPR, TNR, and overall accuracy=
 after self-reflection using the gender bias dataset. The temperature value=
 is set to 1 for text generation. See Appendix Figure\\\\u00a0[9](https://a=
rxiv.org/html/2406.10400v2#A1.F9 \\\\"Figure 9 \\\\u2023 Appendix A Appendi=
x \\\\u2023 Self-Reflection Makes Large Language Models Safer, Less Biased,=
 and Ideologically Neutral\\\\") for each of the four prompts used in exper=
iments.\\\\n\\\\n### 4.4 Self-Reflection Improves Partisan Neutrality\\\\n\=
\\\nIn the fourth experiment, we used a dataset on ideological leaning to t=
est how well self-reflection could tell the difference between LLMs\\\\u201=
9 ideologically partisan and ideologically neutral responses. We evaluated =
how well LLMs can detect politically skewed and moderate responses by calcu=
lating accuracy. Again, the findings are consistent with our earlier experi=
ments on safety and gender bias. As can be seen in Figure\\\\u00a0[4](https=
://arxiv.org/html/2406.10400v2#S4.F4 \\\\"Figure 4 \\\\u2023 4.4 Self-Refle=
ction Improves Partisan Neutrality \\\\u2023 4 Evaluations \\\\u2023 Self-R=
eflection Makes Large Language Models Safer, Less Biased, and Ideologically=
 Neutral\\\\"), we found that LLMs show different levels of self-reflection=
 in their responses. GPT-4o tends to perform better in self-reflection scen=
arios compared to Gemini and Llama. In particular, GPT-4o is able to detect=
 its partisan-aligned responses and enhance the model\\\\u2019s ideological=
 neutrality with a high accuracy of 100%. Additionally, it is still able to=
 detect unaligned or moderate responses with an accuracy of 87.7%. This sho=
ws the remarkable self-reflection capability of GPT-4o. On the other hand, =
both Gemini and Llama exhibit declined performance compared to GPT. However=
, Gemini was able to enhance the model\\\\u2019s ideological neutrality by =
44.0% while keeping false positives so low. Llama increased false positives=
 while improving the detection of partisan-aligned responses. See Appendix =
Figure\\\\u00a0[10](https://arxiv.org/html/2406.10400v2#A1.F10 \\\\"Figure =
10 \\\\u2023 Appendix A Appendix